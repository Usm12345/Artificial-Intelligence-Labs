{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello! This is a sample sentence. How are you? I hope you're doing well. My name is Usman... Nice to meet u \"\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"This is a sample sentence for word tokenization. it will be implemented in the code below : )\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-Speech (POS) Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "sentence = \"This is a sample sentence for POS tagging.\"\n",
    "words = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "sentence = \"An apple a day keeps the Dr. away\"\n",
    "words = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(words)\n",
    "ner_tags = ne_chunk(pos_tags)\n",
    "\n",
    "print(ner_tags)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "word = \"booklet\"\n",
    "synsets = wordnet.synsets(word)\n",
    "\n",
    "print(f\"Synsets of '{word}':\")\n",
    "for synset in synsets:\n",
    "    print(synset.name(), \"-\", synset.definition())\n",
    "    \n",
    "# Get synonyms and antonyms of the word\n",
    "synonyms = set()\n",
    "antonyms = set()\n",
    "\n",
    "for synset in synsets:\n",
    "    for lemma in synset.lemmas():\n",
    "        synonyms.add(lemma.name())\n",
    "        if lemma.antonyms():\n",
    "            antonyms.add(lemma.antonyms()[0].name())\n",
    "\n",
    "print(f\"Synonyms of the '{word}':\", synonyms)\n",
    "print(f\"Antonyms of the '{word}':\", antonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "word = \"tree\"\n",
    "synsets = wordnet.synsets(word)\n",
    "\n",
    "print(f\"Meronyms of '{word}':\")\n",
    "for synset in synsets:\n",
    "    for meronym in synset.part_meronyms():\n",
    "        print(meronym.name(), \"-\", meronym.definition())\n",
    "\n",
    "print(f\"Holonyms of '{word}':\")\n",
    "for synset in synsets:\n",
    "    for holonym in synset.part_holonyms():\n",
    "        print(holonym.name(), \"-\", holonym.definition())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
